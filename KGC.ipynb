{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64fc2ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import random\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c756bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_examples = json.load(open('data/MetaQA/train.txt.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8354247",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_t_dict = {}\n",
    "rt_h_dict = {}\n",
    "def get_triple_dict():\n",
    "    global hr_t_dict\n",
    "    global rt_h_dict\n",
    "    with open('data/MetaQA/train.txt', 'r') as f, open('data/MetaQA/valid.txt', 'r') as f2:\n",
    "        lines = f.readlines() + f2.readlines()\n",
    "        for line in lines:\n",
    "            h, r, t = line.strip().split('\\t')\n",
    "            h, r, t = int(h), int(r), int(t)\n",
    "            if (h, r) in hr_t_dict.keys():\n",
    "                hr_t_dict[(h, r)].append(t)\n",
    "            else:\n",
    "                hr_t_dict[(h, r)] = [t]\n",
    "\n",
    "            # if (t, r + 1) in hr_t_dict.keys():\n",
    "            #     hr_t_dict[(t, r + 1)].append(h)\n",
    "            # else:\n",
    "            #     hr_t_dict[(t, r + 1)] = [h]\n",
    "\n",
    "            if (r, t) in rt_h_dict.keys():\n",
    "                rt_h_dict[(r, t)].append(h)\n",
    "            else:\n",
    "                rt_h_dict[(r, t)] = [h]\n",
    "\n",
    "            # if (r + 1, h) in rt_h_dict.keys():\n",
    "            #     rt_h_dict[(r + 1, h)].append(t)\n",
    "            # else:\n",
    "            #     rt_h_dict[(r + 1, h)] = [t]\n",
    "                \n",
    "get_triple_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "966a2693",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent2idx = {}\n",
    "idx2ent = {}\n",
    "def get_entity_dict():\n",
    "    global ent2idx\n",
    "    global idx2ent\n",
    "    with open('data/MetaQA_data/entities.dict', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            idx, ent = line.strip().split('\\t')\n",
    "            ent2idx[ent] = int(idx)\n",
    "            idx2ent[int(idx)] = ent\n",
    "get_entity_dict()\n",
    "rel2idx = {}\n",
    "idx2rel = {}\n",
    "def get_relation_dict():\n",
    "    global rel2idx\n",
    "    global idx2rel\n",
    "    with open('data/MetaQA_data/relations.dict', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            idx, rel = line.strip().split('\\t')\n",
    "            rel2idx[rel] = int(idx)\n",
    "            idx2rel[int(idx)] = rel\n",
    "get_relation_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbd5c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse(example):\n",
    "    reverse_example = {}\n",
    "    reverse_example['head'] = example['tail']\n",
    "    reverse_example['head_id'] = example['tail_id']\n",
    "    reverse_example['tail'] = example['head']\n",
    "    reverse_example['tail_id'] = example['head_id']\n",
    "    reverse_example['relation'] = example['relation'] + '_reverse'\n",
    "    if 'label' in example.keys():\n",
    "        reverse_example['label'] = example['label']\n",
    "    else:\n",
    "        reverse_example['label'] = 1\n",
    "    return reverse_example\n",
    "\n",
    "def corrupt_examples(example, n=1):\n",
    "    examples = []\n",
    "    h = int(example['head_id'])\n",
    "    r = rel2idx[example['relation']]\n",
    "    true_t = hr_t_dict[(h, r)]\n",
    "    # false_t = set(range(len(ent2idx.keys()))) - true_t\n",
    "    corrupt_ts = []\n",
    "    while len(corrupt_ts) < n:\n",
    "        while True:\n",
    "            rand = random.randint(0, len(ent2idx.keys()) - 1)\n",
    "            if rand not in true_t:\n",
    "                corrupt_ts.append(rand)\n",
    "                break\n",
    "    for corrupt_t in corrupt_ts:\n",
    "        cor_example = {}\n",
    "        cor_example['head'] = example['head']\n",
    "        cor_example['head_id'] = example['head_id']\n",
    "        cor_example['tail'] = idx2ent[corrupt_t]\n",
    "        cor_example['tail_id'] = corrupt_t\n",
    "        cor_example['label'] = 0\n",
    "        cor_example['relation'] = example['relation']\n",
    "        examples.append(cor_example)\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "119a1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KG_Example:\n",
    "    def __init__(self, head, head_id, relation, tail, tail_id, label=1):\n",
    "        self.head = head\n",
    "        self.head_id = int(head_id)\n",
    "        self.relation = relation\n",
    "        self.tail = tail\n",
    "        self.tail_id = int(tail_id)\n",
    "        self.label = label\n",
    "\n",
    "    def vectorize(self):\n",
    "        h_tokenized = tokenizer(text=self.head,\n",
    "                                 max_length=16,\n",
    "                                 return_token_type_ids=True,\n",
    "                                 truncation=True)\n",
    "        \n",
    "        r_tokenized = tokenizer(text=self.relation,\n",
    "                                 max_length=48,\n",
    "                                 return_token_type_ids=True,\n",
    "                                 truncation=True)\n",
    "        \n",
    "        t_tokenized = tokenizer(text=self.tail,\n",
    "                                max_length=16,\n",
    "                                return_token_type_ids=True,\n",
    "                                truncation=True)\n",
    "        \n",
    "        return {\n",
    "                'head_id': self.head_id,\n",
    "                'h_token_ids': h_tokenized['input_ids'],\n",
    "                'h_token_type_ids': h_tokenized['token_type_ids'],\n",
    "                'h_mask': h_tokenized['attention_mask'],\n",
    "                'relation': self.relation,\n",
    "                'r_token_ids': r_tokenized['input_ids'],\n",
    "                'r_token_type_ids': r_tokenized['token_type_ids'],\n",
    "                'r_mask': r_tokenized['attention_mask'],\n",
    "                'tail_id': self.tail_id,\n",
    "                't_token_ids': t_tokenized['input_ids'],\n",
    "                't_token_type_ids': t_tokenized['token_type_ids'],\n",
    "                't_mask': t_tokenized['attention_mask'],\n",
    "                'label': self.label\n",
    "                }\n",
    "    \n",
    "class Ent_Example:\n",
    "    def __init__(self, entity):\n",
    "        self.entity = entity\n",
    "\n",
    "    def vectorize(self):\n",
    "        ent_tokenized = tokenizer(text=self.entity,\n",
    "                                max_length=16,\n",
    "                                return_token_type_ids=True,\n",
    "                                truncation=True)\n",
    "        \n",
    "        return {'ent_token_ids': ent_tokenized['input_ids'],\n",
    "                'ent_token_type_ids': ent_tokenized['token_type_ids'],\n",
    "                'ent_mask': ent_tokenized['attention_mask']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cd76d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KG_Dataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, examples, corrupt_num=2, add_reverse=False):\n",
    "        self.examples = []\n",
    "        for i in tqdm(range(len(examples))):\n",
    "            self.examples.append(KG_Example(**examples[i]))\n",
    "            cor_examples = corrupt_examples(examples[i], n=corrupt_num)\n",
    "            for ex in cor_examples:\n",
    "                self.examples.append(KG_Example(**ex))\n",
    "            if add_reverse:\n",
    "                self.examples.append(KG_Example(**(reverse(examples[i]))))\n",
    "                cor_examples = corrupt_examples(reverse(examples[i]), n=corrupt_num)\n",
    "                for ex in cor_examples:\n",
    "                    self.examples.append(KG_Example(**ex))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.examples[index].vectorize()\n",
    "    \n",
    "class Ent_Dataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = []\n",
    "        for i in range(len(examples)):\n",
    "            self.examples.append(Ent_Example(examples[i]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.examples[index].vectorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19a26bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_batches(batch: list, pad_token_id, return_mask=False):\n",
    "    max_len = max([x.shape[0] for x in batch])\n",
    "    batch_size = len(batch)\n",
    "    token_batch = torch.LongTensor(batch_size, max_len).fill_(pad_token_id)\n",
    "    mask_batch = torch.zeros(batch_size, max_len)\n",
    "    for i, t in enumerate(batch):\n",
    "        token_batch[i, :len(t)].copy_(t)\n",
    "        if return_mask:\n",
    "            mask_batch[i, :len(t)].fill_(1)\n",
    "    if return_mask:\n",
    "        return token_batch, mask_batch\n",
    "    else:\n",
    "        return token_batch\n",
    "\n",
    "def _collate_fn(batch):\n",
    "    batch_dict = {}\n",
    "    batch_dict['h_token_ids'], batch_dict['h_mask'] = padding_batches([torch.LongTensor(x['h_token_ids']) for x in batch],\n",
    "                                                                      tokenizer.pad_token_id,\n",
    "                                                                      return_mask=True)\n",
    "    batch_dict['h_token_type_ids'] = padding_batches([torch.LongTensor(x['h_token_type_ids']) for x in batch], 0)\n",
    "    batch_dict['r_token_ids'], batch_dict['r_mask'] = padding_batches([torch.LongTensor(x['r_token_ids']) for x in batch],\n",
    "                                                                      tokenizer.pad_token_id,\n",
    "                                                                      return_mask=True)\n",
    "    batch_dict['r_token_type_ids'] = padding_batches([torch.LongTensor(x['r_token_type_ids']) for x in batch], 0)\n",
    "    batch_dict['t_token_ids'], batch_dict['t_mask'] = padding_batches([torch.LongTensor(x['t_token_ids']) for x in batch],\n",
    "                                                                      tokenizer.pad_token_id,\n",
    "                                                                      return_mask=True)\n",
    "    batch_dict['t_token_type_ids'] = padding_batches([torch.LongTensor(x['t_token_type_ids']) for x in batch], 0)\n",
    "    batch_dict['label'] = torch.LongTensor([x['label'] for x in batch])\n",
    "    return batch_dict\n",
    "\n",
    "def _collate_fn_ent(batch):\n",
    "    batch_dict = {}\n",
    "    batch_dict['ent_token_ids'], batch_dict['ent_mask'] = padding_batches([torch.LongTensor(x['ent_token_ids']) for x in batch],\n",
    "                                                                          tokenizer.pad_token_id,\n",
    "                                                                          return_mask=True)\n",
    "    batch_dict['ent_token_type_ids'] = padding_batches([torch.LongTensor(x['ent_token_type_ids']) for x in batch], 0)\n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c112ca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133582/133582 [00:01<00:00, 83110.09it/s] \n"
     ]
    }
   ],
   "source": [
    "train_dataset = KG_Dataset(train_examples, corrupt_num=2, add_reverse=False)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=64,\n",
    "                shuffle=True,\n",
    "                collate_fn=_collate_fn,\n",
    "                num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "002ae6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_loader():\n",
    "    ent_list = []\n",
    "    with open('data/MetaQA_data/entities.dict', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            ent_list.append(line.strip().split('\\t')[1])\n",
    "    ent_dataset = Ent_Dataset(ent_list)\n",
    "    ent_dataloader = torch.utils.data.DataLoader(\n",
    "        ent_dataset,\n",
    "        batch_size=1024,\n",
    "        shuffle=False,\n",
    "        collate_fn=_collate_fn_ent,\n",
    "        num_workers=4)\n",
    "    return ent_dataloader\n",
    "ent_dataloader = get_entity_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53ef211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_ent_embedding(model):\n",
    "    ent_embed = []\n",
    "    for ent_batch in ent_dataloader:\n",
    "        batch = move_to_cuda(ent_batch)\n",
    "        batch_embed = model.encode(model.ent_encoder,\n",
    "                                   input_ids=batch['ent_token_ids'],\n",
    "                                   token_type_ids=batch['ent_token_type_ids'],\n",
    "                                   attention_mask=batch['ent_mask'])\n",
    "        ent_embed.append(batch_embed)\n",
    "    return torch.cat(ent_embed)\n",
    "\n",
    "def move_to_cuda(sample):\n",
    "    if len(sample) == 0:\n",
    "        return {}\n",
    "\n",
    "    def _move_to_cuda(maybe_tensor):\n",
    "        if torch.is_tensor(maybe_tensor):\n",
    "            return maybe_tensor.cuda(non_blocking=True)\n",
    "        elif isinstance(maybe_tensor, dict):\n",
    "            return {key: _move_to_cuda(value) for key, value in maybe_tensor.items()}\n",
    "        elif isinstance(maybe_tensor, list):\n",
    "            return [_move_to_cuda(x) for x in maybe_tensor]\n",
    "        elif isinstance(maybe_tensor, tuple):\n",
    "            return [_move_to_cuda(x) for x in maybe_tensor]\n",
    "        else:\n",
    "            return maybe_tensor\n",
    "\n",
    "    return _move_to_cuda(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5c4134ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ent_encoder = AutoModel.from_pretrained('bert-base-uncased').cuda()\n",
    "        self.r_encoder = deepcopy(self.ent_encoder)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.fc = nn.Linear(768, 200)\n",
    "    \n",
    "    def encode(self, model, input_ids, token_type_ids, attention_mask):\n",
    "        embed = model(input_ids=input_ids,\n",
    "                      token_type_ids=token_type_ids,\n",
    "                      attention_mask=attention_mask,\n",
    "                      return_dict=True)\n",
    "        embed = embed.last_hidden_state\n",
    "        embed = self.convert_features(attention_mask, embed)\n",
    "        # embed = self.fc(embed)\n",
    "        return embed\n",
    "\n",
    "    def ComplEx_score(self, h_embed, r_embed, t_embed):\n",
    "        reh, imh = torch.stack(h_embed.chunk(2, dim=1))\n",
    "        rer, imr = torch.stack(r_embed.chunk(2, dim=1))\n",
    "        ret, imt = torch.stack(t_embed.chunk(2, dim=1))\n",
    "        score = (reh*rer*ret + imh*rer*imt + reh*imr*imt - imh*imr*ret).sum(dim=1)\n",
    "        pred = torch.sigmoid(score)\n",
    "        return pred\n",
    "    \n",
    "    def forward(self, batch_dict):\n",
    "        h_embed = self.encode(self.ent_encoder,\n",
    "                              input_ids=batch_dict['h_token_ids'],\n",
    "                              token_type_ids=batch_dict['h_token_type_ids'],\n",
    "                              attention_mask=batch_dict['h_mask'])\n",
    "        r_embed = self.encode(self.r_encoder,\n",
    "                              input_ids=batch_dict['r_token_ids'],\n",
    "                              token_type_ids=batch_dict['r_token_type_ids'],\n",
    "                              attention_mask=batch_dict['r_mask'])\n",
    "        t_embed = self.encode(self.ent_encoder,\n",
    "                              input_ids=batch_dict['t_token_ids'],\n",
    "                              token_type_ids=batch_dict['t_token_type_ids'],\n",
    "                              attention_mask=batch_dict['t_mask'])\n",
    "        return self.ComplEx_score(h_embed, r_embed, t_embed)\n",
    "\n",
    "    def convert_features(self, mask, last_hidden_state, convert_type='cls'):\n",
    "        if convert_type=='cls':\n",
    "            output_vector = last_hidden_state[:,0,:]\n",
    "        else:\n",
    "            input_mask_expanded = mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-4)\n",
    "            output_vector = sum_embeddings / sum_mask\n",
    "        # output_vector = nn.functional.normalize(output_vector, dim=1)\n",
    "        return output_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8d79382b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = KGModel().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fb58da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_rel_embedding(model):\n",
    "    tokenized_rel = tokenizer(text=list(rel2idx.keys()),\n",
    "                                max_length=48,\n",
    "                                return_token_type_ids=True,\n",
    "                                truncation=True)\n",
    "    tokenized_rel['input_ids'] = [torch.LongTensor(x) for x in tokenized_rel['input_ids']]\n",
    "    tokenized_rel['attention_mask'] = [torch.LongTensor(x) for x in tokenized_rel['attention_mask']]\n",
    "    tokenized_rel['token_type_ids'] = [torch.LongTensor(x) for x in tokenized_rel['token_type_ids']]\n",
    "    tokenized_rel['input_ids'] = torch.nn.utils.rnn.pad_sequence(tokenized_rel['input_ids'],\n",
    "                                                                padding_value=tokenizer.pad_token_id,\n",
    "                                                                batch_first=True)\n",
    "    tokenized_rel['attention_mask'] = torch.nn.utils.rnn.pad_sequence(tokenized_rel['attention_mask'],\n",
    "                                                                padding_value=0,\n",
    "                                                                batch_first=True)\n",
    "    tokenized_rel['token_type_ids'] = torch.nn.utils.rnn.pad_sequence(tokenized_rel['token_type_ids'],\n",
    "                                                             padding_value=tokenizer.pad_token_type_id,\n",
    "                                                             batch_first=True)\n",
    "    rel_embed = model.encode(model.r_encoder,\n",
    "                            input_ids=tokenized_rel['input_ids'].cuda(),\n",
    "                            token_type_ids=tokenized_rel['token_type_ids'].cuda(),\n",
    "                            attention_mask=tokenized_rel['attention_mask'].cuda())\n",
    "    return rel_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c5f168f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_mr(model, dataset, ent_embed, rel_embed):\n",
    "    total_mr = 0\n",
    "    total_mrr = 0\n",
    "    n = 0\n",
    "    for data in tqdm(dataset):\n",
    "        h, r, t = data\n",
    "        h_embed = ent_embed[h][None, :]\n",
    "        r_embed = rel_embed[r][None, :]\n",
    "        pred = model.ComplEx_score(h_embed, r_embed, ent_embed)\n",
    "        mr = (pred >= pred[t]).sum()\n",
    "        total_mr += mr.item()\n",
    "        total_mrr += 1 / mr.item()\n",
    "        n += 1\n",
    "    return total_mr / n, total_mrr / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dd223c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluate_dataset():\n",
    "    dataset = []\n",
    "    with open('data/MetaQA/valid.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            h, r, t = line.strip().split('\\t')\n",
    "            h, r, t = int(h), int(r), int(t)\n",
    "            dataset.append([h, r, t])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a46c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_d = get_evaluate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "92ed3f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_dict = move_to_cuda(batch)\n",
    "# # def ComplEx_score(mo, h_embed, r_embed, t_embed):\n",
    "# #     reh, imh = torch.stack(h_embed.chunk(2, dim=1))\n",
    "# #     rer, imr = torch.stack(r_embed.chunk(2, dim=1))\n",
    "# #     ret, imt = torch.stack(t_embed.chunk(2, dim=1))\n",
    "# #     score = (reh*rer*ret + imh*rer*imt + reh*imr*imt - imh*imr*ret).sum(dim=1)\n",
    "# #     pred = torch.sigmoid(score)\n",
    "# #     return pred\n",
    "\n",
    "\n",
    "# h_embed = model.encode(model.ent_encoder,\n",
    "#                         input_ids=batch_dict['h_token_ids'],\n",
    "#                         token_type_ids=batch_dict['h_token_type_ids'],\n",
    "#                         attention_mask=batch_dict['h_mask'])\n",
    "# r_embed = model.encode(model.r_encoder,\n",
    "#                         input_ids=batch_dict['r_token_ids'],\n",
    "#                         token_type_ids=batch_dict['r_token_type_ids'],\n",
    "#                         attention_mask=batch_dict['r_mask'])\n",
    "# t_embed = model.encode(model.ent_encoder,\n",
    "#                         input_ids=batch_dict['t_token_ids'],\n",
    "#                         token_type_ids=batch_dict['t_token_type_ids'],\n",
    "#                         attention_mask=batch_dict['t_mask'])\n",
    "# rel_embed[:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d9aa2f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.2673, avg loss:0.286:  85%|████████▍ | 5292/6262 [10:47<01:58,  8.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m loss \u001b[39m=\u001b[39m loss_f(x, label\u001b[39m.\u001b[39mcuda() \u001b[39m*\u001b[39m \u001b[39m1.0\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     14\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     15\u001b[0m avg_loss \u001b[39m=\u001b[39m total_loss \u001b[39m/\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_f = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=1e-4)\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    with tqdm(total=len(train_dataloader)) as pbar:\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            x = model(move_to_cuda(batch))\n",
    "            label = batch['label']\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_f(x, label.cuda() * 1.0)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            avg_loss = total_loss / (i + 1)\n",
    "            optimizer.step()\n",
    "            pbar.set_description(f\"loss:{loss.item():.4f}, avg loss:{avg_loss:.3f}\")\n",
    "            pbar.update(1)\n",
    "    print('Evaluating Model')\n",
    "    model.eval()\n",
    "    ent_embed = get_ent_embedding(model)\n",
    "    rel_embed = get_rel_embedding(model)\n",
    "    # acc = evaluate_accuracy(model, valid_dataloader)\n",
    "    mr, mrr = evaluate_mr(model, valid_d, ent_embed, rel_embed)\n",
    "    print(f'mr: {mr:.2f}, mrr: {mrr:.2f}')\n",
    "torch.save(model.state_dict(), f'save_model/kgmodel_mr{mr:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4032f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "1800 0.265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "00115e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4053/4053 [00:07<00:00, 523.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(23240.984702689366, 9.267469350604106e-05)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_embed = get_ent_embedding(model)\n",
    "rel_embed = get_rel_embedding(model)\n",
    "evaluate_mr(model, valid_d, ent_embed, rel_embed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
